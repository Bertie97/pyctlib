try:
    import torch
except ImportError:
    raise ImportError("'pyctlib.torchplus' cannot be used without dependency 'torch'.")
import torch.nn as nn
from pyctlib.basictype import vector, return_type_wrapper, raw_function
from functools import wraps
from pyctlib.override import override
from pyctlib.exception import *

Device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

def return_tensor_wrapper(func):
    func = raw_function(func)
    @wraps(func)
    def wrapper(*args, **kwargs):
        return Tensor(func(*args, **kwargs))
    return wrapper

class Tensor(torch.Tensor):

    wrapper = return_tensor_wrapper

    def __new__(cls, data, dtype=None, device=None, requires_grad=False, batch_dimension=None, auto_device=True):
        self = super().__new__(cls, data)
        if dtype is not None:
            self.type(dtype)
        if device is not None:
            self.to(device)
        elif auto_device:
            self.to(Device)
        self.requires_grad = requires_grad
        self._auto_device=auto_device
        self._batch_dimension = batch_dimension
        return self

    def __init__(self, *args, **kwargs):
        pass

    @property
    def batch_dimension(self):
        return self._batch_dimension

    @property
    def batch_size(self):
        if self.batch_dimension is None:
            raise ValueError("there is no dimension provided for this tensor")
        return self.shape[self.batch_dimension]

    def numpy(self: 'Tensor'):
        return self.cpu().detach().numpy()

    @property
    @return_tensor_wrapper
    def T(self: 'Tensor') -> 'Tensor':
        if self.batch_dimension is None:
            return super().T
        # return torch.transpose((torch.unsqueeze(self).transpose(0, self.batch_dimension+1).T), -  V1
        # return torch.permute
        permute_dim = tuple(range(self.batch_dimension))[::-1] + (self.batch_dimension,) + tuple(range(self.batch_dimension+1, self.dim()))[::-1]
        return self.permute(permute_dim)

    def test(self, a):
        return a + 1

x = Tensor([1,2,3])
def getdoc(func):
    assert func in x.__dir__()
    try:
        exec("torch_func = torch.%s" % func)
    except:
        pass
    torch_func = touch(lambda: torch.__dict__[func].__doc__)
    class_func = touch(lambda: x.__getattribute__(func).__doc__)
    return torch_func, class_func, func

import re

def getgooddoc(func):
    torch_func, class_func, func = getdoc(func)
    if not torch_func and not class_func:
        return ""
    if class_func:
        if touch(lambda:class_func.split("\n")[-2].startswith("See :func:")):
            new_func = re.match(r"See :func:`([\w.]*)`", class_func.split("\n")[-2]).group(1)
            # print(new_func)
            torch_func = eval(new_func + ".__doc__")
            # exec("torch_func = " + new_func + ".__doc__")
            # print(torch_func, "torch_func = " + new_func)
            return torch_func
        else:
            return class_func
    return torch_func

p = vector(x.__dir__()).filter(lambda x: x[0] != "_")
